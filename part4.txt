Prompt:  
Some time has passed since this system went live and now there are 50,000,000 records in the books table. Querying books is now taking a very long time and timeouts are occurring. What steps would you take to solve this problem?




Answer: 
The first step I would take is to assess the access patterns of the table. How is the data being queried? Based on that information, I would ensure the table has the correct indexing in place to optimize those access patterns. For example, if we are frequently looking up books based on the isbn then I would likely have that column indexed. Based on how I created the books table and created indexes in my SQL script I should have the columns with the most frequent access patterns indexed already. If other access patterns existed outside of those indexes then we could consider indexing the respective columns if needed but we would want to ensure it makes sense based on the cardinality of the column’s data, maintenance costs, etc.

Next, I would tackle the scaling data size. For that, we could consider partitioning so long as the database we are using supports this functionality. One potential approach would be to partition by date. For the books table, we could look at partitioning based on publication date and have it partitioned by decade. So in this case, each partition would contain the books from a respective decade. Then, partition 1 stores books published in 1990-2000, partition 2 holds books published in 2000-2010, etc. I would trial different date ranges to ensure we have reasonable data sizes in each partition.

Note that in PostgreSQL, we can’t partition directly onto an existing table. Ideally, we would have the partitioning built in when the table was created. But if we are creating the partitions after the fact, which I am assuming is the case based on the prompt, then we would need to create a new partitioned table along with the partitions based on our criteria. Then we would need to transfer the data over to the partitioned table. From there, we could delete the old table and rename our new partitioned table if desired. We would want to be sure data is backed up before starting this process. There will also be downtime while this process is taking place, which should be accounted for. Then after the fact, we would want to test querying to ensure accuracy and that query performance has improved.

Another approach to consider would be to archive older data. For this use case, we could create an archive table that stores old books that are no longer active or in stock. One approach would be to add an ‘active’ column to our books table (boolean) that would tell us if the book is active or not. Then, we could create a trigger that moves books that are non-active to the archive table. Then the books table would only hold active books which would hopefully free up space and enhance querying operations. Note, that we could also set up a trigger to move books from the archive table back to the books table if activated.